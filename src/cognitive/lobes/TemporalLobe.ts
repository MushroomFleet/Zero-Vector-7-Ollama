import { ElevenLabsService, TTSResponse } from '../../services/elevenLabsService';
import { SessionContext } from '../../types/cognitive';

export interface SpeechTrace {
  originalText: string;
  speechSummary: string;
  audioGenerated: boolean;
  audioUrl?: string;
  processingTime: number;
  voiceUsed: string;
}

export class TemporalLobe {
  private elevenLabsService: ElevenLabsService | null = null;
  private audioCache = new Map<string, string>();

  constructor(apiKey?: string) {
    if (apiKey) {
      this.elevenLabsService = new ElevenLabsService(apiKey);
    }
  }

  // Speech summary is now generated by Inner Thoughts, this just receives it
  receiveSpeechSummary(speechSummary: string): string {
    return speechSummary;
  }

  async synthesizeText(
    text: string,
    voiceId: string,
    voiceSettings: any
  ): Promise<SpeechTrace> {
    const startTime = Date.now();
    
    if (!this.elevenLabsService) {
      return {
        originalText: text,
        speechSummary: text,
        audioGenerated: false,
        processingTime: Date.now() - startTime,
        voiceUsed: 'none'
      };
    }

    try {
      // Use the text directly as it's already been processed by Inner Thoughts
      const speechSummary = text;
      
      // Check cache first
      const cacheKey = `${speechSummary}-${voiceId}`;
      if (this.audioCache.has(cacheKey)) {
        return {
          originalText: text,
          speechSummary,
          audioGenerated: true,
          audioUrl: this.audioCache.get(cacheKey),
          processingTime: Date.now() - startTime,
          voiceUsed: voiceId
        };
      }

      // Generate new audio
      const ttsResponse = await this.elevenLabsService.textToSpeech(
        speechSummary,
        voiceId,
        voiceSettings
      );

      // Cache the result
      this.audioCache.set(cacheKey, ttsResponse.audioUrl);

      return {
        originalText: text,
        speechSummary,
        audioGenerated: true,
        audioUrl: ttsResponse.audioUrl,
        processingTime: Date.now() - startTime,
        voiceUsed: voiceId
      };

    } catch (error) {
      console.error('Text-to-speech synthesis failed:', error);
      return {
        originalText: text,
        speechSummary: text,
        audioGenerated: false,
        processingTime: Date.now() - startTime,
        voiceUsed: voiceId
      };
    }
  }

  async synthesizeFullText(
    text: string,
    voiceId: string,
    voiceSettings: any
  ): Promise<TTSResponse | null> {
    if (!this.elevenLabsService) return null;

    try {
      return await this.elevenLabsService.textToSpeech(text, voiceId, voiceSettings);
    } catch (error) {
      console.error('Full text synthesis failed:', error);
      return null;
    }
  }

  private generateFallbackSummary(text: string, maxLength: number): string {
    if (text.length <= maxLength) return text;

    // Simple sentence-based summarization
    const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 10);
    let summary = '';
    
    for (const sentence of sentences) {
      if (summary.length + sentence.length + 2 <= maxLength) {
        summary += sentence.trim() + '. ';
      } else {
        break;
      }
    }

    return summary.trim() || text.substring(0, maxLength - 3) + '...';
  }

  clearAudioCache(): void {
    // Revoke object URLs to prevent memory leaks
    this.audioCache.forEach(url => URL.revokeObjectURL(url));
    this.audioCache.clear();
  }

  getStoredAudioFiles(): Map<string, string> {
    return new Map(this.audioCache);
  }

  updateElevenLabsKey(apiKey: string): void {
    this.elevenLabsService = new ElevenLabsService(apiKey);
  }
}